{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvRPMQreajMk",
        "outputId": "876e42a7-c4e6-4fc2-febe-76164fd3d6b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=a414b8babf68a3fe39981ef8b7552a388746c915f250edba5f6610dfe9441f4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"heart_disease\").getOrCreate()"
      ],
      "metadata": {
        "id": "Y404ip00aott"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"datasets_heart.csv\", inferSchema=True, header=True)\n",
        "df.show(2)\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejHd-ec_a3u-",
        "outputId": "3e2fc61b-485c-4ac6-e893-12c375c5358c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
            "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7PJH1qla9CX",
        "outputId": "da770aa4-6f86-403e-c82f-87fbac1a413d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('target').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKN9opZVbCk6",
        "outputId": "ebc641f6-5d8c-49c0-b401-76417c541c35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     1|  165|\n",
            "|     0|  138|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "def null_value_calc(df):\n",
        "    null_columns_counts = []\n",
        "    numRows = df.count()\n",
        "    for k in df.columns:\n",
        "        nullRows = df.where(col(k).isNull()).count()\n",
        "        if(nullRows > 0):\n",
        "            temp = k,nullRows,(nullRows/numRows)*100\n",
        "            null_columns_counts.append(temp)\n",
        "    return(null_columns_counts)\n",
        "\n",
        "null_columns_calc_list = null_value_calc(df)\n",
        "if null_columns_calc_list :\n",
        "    spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()\n",
        "else :\n",
        "    print(\"Data is clean with no null values\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTcr_qycbEiS",
        "outputId": "aed0a4ac-f634-48c0-9835-6624fe00f047"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is clean with no null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in dependencies\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ],
      "metadata": {
        "id": "BPLjTj0BbHzb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Prep function\n",
        "def MLClassifierDFPrep(df,input_columns,dependent_var,treat_outliers=True,treat_neg_values=True):\n",
        "\n",
        "    # change label (class variable) to string type to prep for reindexing\n",
        "    # Pyspark is expecting a zero indexed integer for the label column.\n",
        "    # Just incase our data is not in that format... we will treat it by using the StringIndexer built in method\n",
        "    renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
        "    indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention\n",
        "    indexed = indexer.fit(renamed).transform(renamed)\n",
        "    print(indexed.groupBy(dependent_var,\"label\").count().show(100))\n",
        "\n",
        "    # Convert all string type data in the input column list to numeric\n",
        "    # Otherwise the Algorithm will not be able to process it\n",
        "    numeric_inputs = []\n",
        "    string_inputs = []\n",
        "    for column in input_columns:\n",
        "        if str(indexed.schema[column].dataType) == 'StringType':\n",
        "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\")\n",
        "            indexed = indexer.fit(indexed).transform(indexed)\n",
        "            new_col_name = column+\"_num\"\n",
        "            string_inputs.append(new_col_name)\n",
        "        else:\n",
        "            numeric_inputs.append(column)\n",
        "\n",
        "    if treat_outliers == True:\n",
        "        print(\"We are correcting for non normality now!\")\n",
        "        # empty dictionary d\n",
        "        d = {}\n",
        "        # Create a dictionary of quantiles\n",
        "        for col in numeric_inputs:\n",
        "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
        "        #Now fill in the values\n",
        "        for col in numeric_inputs:\n",
        "            skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
        "            skew = skew[0][0]\n",
        "            # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
        "            if skew > 1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                log(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] ) +1).alias(col))\n",
        "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
        "            elif skew < -1:\n",
        "                indexed = indexed.withColumn(col, \\\n",
        "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
        "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
        "                .otherwise(indexed[col] )).alias(col))\n",
        "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
        "\n",
        "\n",
        "    # Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used.\n",
        "    # Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
        "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the mins for all columns in the df\n",
        "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) # Create an array for all mins and select only the input cols\n",
        "    df_minimum = min_array.select(array_min(min_array.mins)).collect() # Collect golobal min as Python object\n",
        "    df_minimum = df_minimum[0][0] # Slice to get the number itself\n",
        "\n",
        "    features_list = numeric_inputs + string_inputs\n",
        "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
        "    output = assembler.transform(indexed).select('features','label')\n",
        "\n",
        "#     final_data = output.select('features','label') #drop everything else\n",
        "\n",
        "    # Now check for negative values and ask user if they want to correct that?\n",
        "    if df_minimum < 0:\n",
        "        print(\" \")\n",
        "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
        "        print(\" \")\n",
        "\n",
        "    if treat_neg_values == True:\n",
        "        print(\"You have opted to correct that by rescaling all your features to a range of 0 to 1\")\n",
        "        print(\" \")\n",
        "        print(\"We are rescaling you dataframe....\")\n",
        "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "        # Compute summary statistics and generate MinMaxScalerModel\n",
        "        scalerModel = scaler.fit(output)\n",
        "\n",
        "        # rescale each feature to range [min, max].\n",
        "        scaled_data = scalerModel.transform(output)\n",
        "        final_data = scaled_data.select('label','scaledFeatures') # added class to the selection\n",
        "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
        "        print(\"Done!\")\n",
        "\n",
        "    else:\n",
        "        print(\"You have opted not to correct that therefore you will not be able to use to Naive Bayes classifier\")\n",
        "        print(\"We will return the dataframe unscaled.\")\n",
        "        final_data = output\n",
        "\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "hfXa9PJKbMBa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ClassTrainEval(classifier,features,classes,folds,train,test):\n",
        "\n",
        "    def FindMtype(classifier):\n",
        "        # Intstantiate Model\n",
        "        M = classifier\n",
        "        # Learn what it is\n",
        "        Mtype = type(M).__name__\n",
        "\n",
        "        return Mtype\n",
        "\n",
        "    Mtype = FindMtype(classifier)\n",
        "\n",
        "\n",
        "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
        "\n",
        "        if Mtype == \"OneVsRest\":\n",
        "            # instantiate the base classifier.\n",
        "            lr = LogisticRegression()\n",
        "            # instantiate the One Vs Rest Classifier.\n",
        "            OVRclassifier = OneVsRest(classifier=lr)\n",
        "#             fitModel = OVRclassifier.fit(train)\n",
        "            # Add parameters of your choice here:\n",
        "            paramGrid = ParamGridBuilder() \\\n",
        "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "                .build()\n",
        "            #Cross Validator requires the following parameters:\n",
        "            crossval = CrossValidator(estimator=OVRclassifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 is best practice\n",
        "            # Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
        "            # specify layers for the neural network:\n",
        "            # input layer of size features, two intermediate of features+1 and same size as features\n",
        "            # and output of size number of classes\n",
        "            # Note: crossvalidator cannot be used here\n",
        "            features_count = len(features[0][0])\n",
        "            layers = [features_count, features_count+1, features_count, classes]\n",
        "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "            fitModel = MPC_classifier.fit(train)\n",
        "            return fitModel\n",
        "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
        "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
        "            return\n",
        "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"LogisticRegression\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
        "                             .build())\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"NaiveBayes\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
        "                             .build())\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"RandomForestClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
        "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
        "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
        "                             .build())\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"GBTClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
        "                             .build())\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"LinearSVC\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
        "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
        "                             .build())\n",
        "\n",
        "            # Add parameters of your choice here:\n",
        "            if Mtype in(\"DecisionTreeClassifier\"):\n",
        "                paramGrid = (ParamGridBuilder() \\\n",
        "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
        "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "                             .build())\n",
        "\n",
        "            #Cross Validator requires all of the following parameters:\n",
        "            crossval = CrossValidator(estimator=classifier,\n",
        "                                      estimatorParamMaps=paramGrid,\n",
        "                                      evaluator=MulticlassClassificationEvaluator(),\n",
        "                                      numFolds=folds) # 3 + is best practice\n",
        "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
        "            fitModel = crossval.fit(train)\n",
        "            return fitModel\n",
        "\n",
        "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
        "\n",
        "    # Print feature selection metrics\n",
        "    if fitModel is not None:\n",
        "\n",
        "        if Mtype in(\"OneVsRest\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            global OVR_BestModel\n",
        "            OVR_BestModel = BestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            # Extract list of binary models\n",
        "            models = BestModel.models\n",
        "            for model in models:\n",
        "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept)\n",
        "                print('\\033[1m' + 'Top 20 Coefficients:'+ '\\033[0m')\n",
        "                coeff_array = model.coefficients.toArray()\n",
        "                coeff_scores = []\n",
        "                for x in coeff_array:\n",
        "                    coeff_scores.append(float(x))\n",
        "                # Then zip with input_columns list and create a df\n",
        "                result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "                print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "\n",
        "\n",
        "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
        "            print(\"\")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
        "            print(\"\")\n",
        "            global MLPC_Model\n",
        "            MLPC_BestModel = fitModel\n",
        "\n",
        "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
        "            # FEATURE IMPORTANCES\n",
        "            # Estimate of the importance of each feature.\n",
        "            # Each feature’s importance is the average of its importance across all trees\n",
        "            # in the ensemble The importance vector is normalized to sum to 1.\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
        "            print(\"(Scores add up to 1)\")\n",
        "            print(\"Lowest score is the least important\")\n",
        "            print(\" \")\n",
        "            featureImportances = BestModel.featureImportances.toArray()\n",
        "            # Convert from numpy array to list\n",
        "            imp_scores = []\n",
        "            for x in featureImportances:\n",
        "                imp_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
        "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
        "\n",
        "            # Save the feature importance values and the models\n",
        "            if Mtype in(\"DecisionTreeClassifier\"):\n",
        "                global DT_featureimportances\n",
        "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global DT_BestModel\n",
        "                DT_BestModel = BestModel\n",
        "            if Mtype in(\"GBTClassifier\"):\n",
        "                global GBT_featureimportances\n",
        "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global GBT_BestModel\n",
        "                GBT_BestModel = BestModel\n",
        "            if Mtype in(\"RandomForestClassifier\"):\n",
        "                global RF_featureimportances\n",
        "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
        "                global RF_BestModel\n",
        "                RF_BestModel = BestModel\n",
        "\n",
        "        # Print the coefficients\n",
        "        if Mtype in(\"LogisticRegression\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
        "            print('\\033[1m' + \" Top 20 Coefficients\"+ '\\033[0m')\n",
        "            print(\"You should compares these relative to eachother\")\n",
        "            # Convert from numpy array to list\n",
        "            coeff_array = BestModel.coefficientMatrix.toArray()\n",
        "            coeff_scores = []\n",
        "            for x in coeff_array[0]:\n",
        "                coeff_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "            # Save the coefficient values and the models\n",
        "            global LR_coefficients\n",
        "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
        "            global LR_BestModel\n",
        "            LR_BestModel = BestModel\n",
        "\n",
        "        # Print the Coefficients\n",
        "        if Mtype in(\"LinearSVC\"):\n",
        "            # Get Best Model\n",
        "            BestModel = fitModel.bestModel\n",
        "            print(\" \")\n",
        "            print('\\033[1m' + Mtype + '\\033[0m')\n",
        "            print(\"Intercept: \" + str(BestModel.intercept))\n",
        "            print('\\033[1m' + \"Top 20 Coefficients\"+ '\\033[0m')\n",
        "            print(\"You should compares these relative to eachother\")\n",
        "#             print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
        "            coeff_array = BestModel.coefficients.toArray()\n",
        "            coeff_scores = []\n",
        "            for x in coeff_array:\n",
        "                coeff_scores.append(float(x))\n",
        "            # Then zip with input_columns list and create a df\n",
        "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
        "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
        "            # Save the coefficient values and the models\n",
        "            global LSVC_coefficients\n",
        "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
        "            global LSVC_BestModel\n",
        "            LSVC_BestModel = BestModel\n",
        "\n",
        "\n",
        "    # Set the column names to match the external results dataframe that we will join with later:\n",
        "    columns = ['Classifier', 'Result']\n",
        "\n",
        "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
        "        Mtype = [Mtype] # make this a list\n",
        "        score = [\"N/A\"]\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "    else:\n",
        "        predictions = fitModel.transform(test)\n",
        "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
        "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
        "        Mtype = [Mtype] # make this a string\n",
        "        score = [str(accuracy)] #make this a string and convert to a list\n",
        "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
        "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
        "\n",
        "    return result\n",
        "    #Also returns the fit model important scores or p values"
      ],
      "metadata": {
        "id": "HZd3qLnlbOir"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up independ and dependent vars\n",
        "input_columns = df.columns\n",
        "input_columns = input_columns[:-1] # keep only relevant columns: everything but the first and last cols\n",
        "dependent_var = 'target'\n",
        "\n",
        "# Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
        "class_count = df.select(countDistinct(\"target\")).collect()\n",
        "classes = class_count[0][0]"
      ],
      "metadata": {
        "id": "Jnbj4mZ_bUDP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call on data prep, train and evaluate functions\n",
        "test1_data = MLClassifierDFPrep(df,input_columns,dependent_var)\n",
        "test1_data.limit(5).toPandas()\n",
        "\n",
        "# Comment out Naive Bayes if your data still contains negative values\n",
        "classifiers = [\n",
        "                LogisticRegression()\n",
        "                ,OneVsRest()\n",
        "               ,LinearSVC()\n",
        "               ,NaiveBayes()\n",
        "               ,RandomForestClassifier()\n",
        "               ,GBTClassifier()\n",
        "               ,DecisionTreeClassifier()\n",
        "               ,MultilayerPerceptronClassifier()\n",
        "              ]\n",
        "\n",
        "train,test = test1_data.randomSplit([0.8,0.2])\n",
        "features = test1_data.select(['features']).collect()\n",
        "folds = 3 # because we have limited data\n",
        "\n",
        "#set up your results table\n",
        "columns = ['Classifier', 'Result']\n",
        "vals = [(\"Place Holder\",\"N/A\")]\n",
        "results = spark.createDataFrame(vals, columns)\n",
        "\n",
        "for classifier in classifiers:\n",
        "    new_result = ClassTrainEval(classifier,features,classes,folds,train,test)\n",
        "    results = results.union(new_result)\n",
        "results = results.where(\"Classifier!='Place Holder'\")\n",
        "print(\"!!!!!Final Results!!!!!!!!\")\n",
        "results.show(100,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PICTitJbYU_",
        "outputId": "ff9c9beb-4d7f-4d60-99bb-1ed9bfae5469"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|target|label|count|\n",
            "+------+-----+-----+\n",
            "|     1|  0.0|  165|\n",
            "|     0|  1.0|  138|\n",
            "+------+-----+-----+\n",
            "\n",
            "None\n",
            "We are correcting for non normality now!\n",
            "chol has been treated for positive (right) skewness. (skew =) 1.1377326187082237 )\n",
            "fbs has been treated for positive (right) skewness. (skew =) 1.9768034646834516 )\n",
            "oldpeak has been treated for positive (right) skewness. (skew =) 1.2634255245891595 )\n",
            "ca has been treated for positive (right) skewness. (skew =) 1.303925955673585 )\n",
            "You have opted to correct that by rescaling all your features to a range of 0 to 1\n",
            " \n",
            "We are rescaling you dataframe....\n",
            "Done!\n",
            " \n",
            "\u001b[1mLogisticRegression\u001b[0m\n",
            "Intercept: [-3.420600852742597]\n",
            "\u001b[1m Top 20 Coefficients\u001b[0m\n",
            "You should compares these relative to eachother\n",
            "+--------+-------------------+\n",
            "|feature |coeff              |\n",
            "+--------+-------------------+\n",
            "|thal    |3.3667064241094367 |\n",
            "|ca      |3.228716282713848  |\n",
            "|trestbps|2.8689748217865527 |\n",
            "|chol    |2.2639295760167464 |\n",
            "|oldpeak |2.039453125635616  |\n",
            "|sex     |1.5873103968655573 |\n",
            "|exang   |0.5987772953923854 |\n",
            "|age     |0.06083548163747581|\n",
            "|fbs     |-0.3979401365070486|\n",
            "|restecg |-1.0065454143119272|\n",
            "|slope   |-1.63993810423012  |\n",
            "|cp      |-2.6911267952056903|\n",
            "|thalach |-2.81139588697509  |\n",
            "+--------+-------------------+\n",
            "\n",
            "None\n",
            " \n",
            "\u001b[1mOneVsRest\u001b[0m\n",
            "\u001b[1mIntercept: \u001b[0m 3.0574429434982737\n",
            "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
            "+--------+-------------------+\n",
            "|feature |coeff              |\n",
            "+--------+-------------------+\n",
            "|thalach |2.4729434403274135 |\n",
            "|cp      |2.294208850759704  |\n",
            "|slope   |1.4058650346042212 |\n",
            "|restecg |0.8682708156911805 |\n",
            "|fbs     |0.3132108071835397 |\n",
            "|age     |-0.2526419201758093|\n",
            "|exang   |-0.613734492052002 |\n",
            "|sex     |-1.3502385851792786|\n",
            "|chol    |-1.7868847790664208|\n",
            "|oldpeak |-1.8127506181857216|\n",
            "|trestbps|-2.329429698397647 |\n",
            "|ca      |-2.7475627574012695|\n",
            "|thal    |-2.9898784085882397|\n",
            "+--------+-------------------+\n",
            "\n",
            "None\n",
            "\u001b[1mIntercept: \u001b[0m -3.057442943498276\n",
            "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
            "+--------+-------------------+\n",
            "|feature |coeff              |\n",
            "+--------+-------------------+\n",
            "|thal    |2.989878408588254  |\n",
            "|ca      |2.74756275740127   |\n",
            "|trestbps|2.3294296983976452 |\n",
            "|oldpeak |1.8127506181857216 |\n",
            "|chol    |1.786884779066425  |\n",
            "|sex     |1.3502385851792762 |\n",
            "|exang   |0.6137344920519999 |\n",
            "|age     |0.2526419201758027 |\n",
            "|fbs     |-0.3132108071835393|\n",
            "|restecg |-0.8682708156911805|\n",
            "|slope   |-1.4058650346042212|\n",
            "|cp      |-2.2942088507597043|\n",
            "|thalach |-2.47294344032742  |\n",
            "+--------+-------------------+\n",
            "\n",
            "None\n",
            " \n",
            "\u001b[1mLinearSVC\u001b[0m\n",
            "Intercept: -2.8584410835405096\n",
            "\u001b[1mTop 20 Coefficients\u001b[0m\n",
            "You should compares these relative to eachother\n",
            "+--------+--------------------+\n",
            "|feature |coeff               |\n",
            "+--------+--------------------+\n",
            "|thal    |2.7526465178211983  |\n",
            "|trestbps|2.0320764820960555  |\n",
            "|ca      |1.9639043896005153  |\n",
            "|oldpeak |1.4640846202927738  |\n",
            "|chol    |1.2803716433116532  |\n",
            "|sex     |0.9052799900722162  |\n",
            "|exang   |0.36781153368070285 |\n",
            "|age     |0.010738820992803216|\n",
            "|restecg |-0.2648421360086167 |\n",
            "|fbs     |-0.35091668953124505|\n",
            "|slope   |-1.0630643971633218 |\n",
            "|thalach |-1.5641495251417559 |\n",
            "|cp      |-1.884964880689476  |\n",
            "+--------+--------------------+\n",
            "\n",
            "None\n",
            " \n",
            "\u001b[1mRandomForestClassifier  Top 20 Feature Importances\u001b[0m\n",
            "(Scores add up to 1)\n",
            "Lowest score is the least important\n",
            " \n",
            "+--------+--------------------+\n",
            "|feature |score               |\n",
            "+--------+--------------------+\n",
            "|oldpeak |0.15086603954213523 |\n",
            "|ca      |0.1284933687630892  |\n",
            "|cp      |0.12506246641725224 |\n",
            "|age     |0.1107091854400483  |\n",
            "|thalach |0.10185787508506165 |\n",
            "|thal    |0.09692234701619007 |\n",
            "|chol    |0.08610642329469091 |\n",
            "|trestbps|0.06118486734639613 |\n",
            "|exang   |0.06036945011413258 |\n",
            "|slope   |0.03695874713823497 |\n",
            "|sex     |0.022013700558571835|\n",
            "|restecg |0.013100942210231892|\n",
            "|fbs     |0.006354587073964914|\n",
            "+--------+--------------------+\n",
            "\n",
            "None\n",
            " \n",
            "\u001b[1mGBTClassifier  Top 20 Feature Importances\u001b[0m\n",
            "(Scores add up to 1)\n",
            "Lowest score is the least important\n",
            " \n",
            "+--------+--------------------+\n",
            "|feature |score               |\n",
            "+--------+--------------------+\n",
            "|cp      |0.1489641298438602  |\n",
            "|age     |0.14489245599621187 |\n",
            "|ca      |0.11723930172329612 |\n",
            "|slope   |0.11373609357536305 |\n",
            "|thalach |0.10780321756287868 |\n",
            "|oldpeak |0.0947515297895842  |\n",
            "|trestbps|0.07568871246291121 |\n",
            "|chol    |0.07365603573512366 |\n",
            "|thal    |0.057462013886823024|\n",
            "|sex     |0.03703386378919394 |\n",
            "|fbs     |0.01194810526606278 |\n",
            "|restecg |0.009494843228196987|\n",
            "|exang   |0.007329697140494253|\n",
            "+--------+--------------------+\n",
            "\n",
            "None\n",
            " \n",
            "\u001b[1mDecisionTreeClassifier  Top 20 Feature Importances\u001b[0m\n",
            "(Scores add up to 1)\n",
            "Lowest score is the least important\n",
            " \n",
            "+--------+--------------------+\n",
            "|feature |score               |\n",
            "+--------+--------------------+\n",
            "|cp      |0.31472766540251623 |\n",
            "|ca      |0.1576501319734585  |\n",
            "|age     |0.1330477144278151  |\n",
            "|thal    |0.10985238757787268 |\n",
            "|oldpeak |0.10444698491735485 |\n",
            "|sex     |0.08151212079212314 |\n",
            "|thalach |0.05204941584082629 |\n",
            "|chol    |0.0190530653463817  |\n",
            "|exang   |0.013849280597910752|\n",
            "|trestbps|0.013811233123740696|\n",
            "|restecg |0.0                 |\n",
            "|fbs     |0.0                 |\n",
            "|slope   |0.0                 |\n",
            "+--------+--------------------+\n",
            "\n",
            "None\n",
            "\n",
            "\u001b[1mMultilayerPerceptronClassifier\u001b[0m\n",
            "\u001b[1mModel Weights: \u001b[0m 419\n",
            "\n",
            "!!!!!Final Results!!!!!!!!\n",
            "+------------------------------+------+\n",
            "|Classifier                    |Result|\n",
            "+------------------------------+------+\n",
            "|LogisticRegression            |88.67 |\n",
            "|OneVsRest                     |88.67 |\n",
            "|LinearSVC                     |90.56 |\n",
            "|NaiveBayes                    |86.79 |\n",
            "|RandomForestClassifier        |90.56 |\n",
            "|GBTClassifier                 |73.58 |\n",
            "|DecisionTreeClassifier        |73.58 |\n",
            "|MultilayerPerceptronClassifier|75.47 |\n",
            "+------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = LR_BestModel.transform(test)"
      ],
      "metadata": {
        "id": "irgGOEZpbiog"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.filter(\"prediction==1\").show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQHv27IddOrl",
        "outputId": "0758bdfb-a810-43eb-b8eb-4b0dfd0d03aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|[0.60416666666666...|[-3.0388330110805...|[0.04570204012186...|       1.0|\n",
            "|  0.0|[0.77083333333333...|[-0.1719789044170...|[0.45711093173763...|       1.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.groupBy(\"label\").count().show()\n",
        "predictions.groupBy(\"prediction\").count().show()\n",
        "\n",
        "predictions.filter(\"prediction != label\").count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3ELMtpbdQ73",
        "outputId": "c5ed69af-2c4e-4429-fe45-0cec9f9d0587"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|   29|\n",
            "|  1.0|   24|\n",
            "+-----+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|       0.0|   31|\n",
            "|       1.0|   22|\n",
            "+----------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorSlicer\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "classifiers = [ LogisticRegression()\n",
        "                ]\n",
        "\n",
        "#Select the top n features and view results\n",
        "n = 99\n",
        "\n",
        "# For Logistic regression or One vs Rest\n",
        "selector = ChiSqSelector(numTopFeatures=n, featuresCol=\"features\",\n",
        "                     outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
        "bestFeaturesDf = selector.fit(test1_data).transform(test1_data)\n",
        "bestFeaturesDf = bestFeaturesDf.select(\"label\",\"selectedFeatures\")\n",
        "bestFeaturesDf = bestFeaturesDf.withColumnRenamed(\"selectedFeatures\",\"features\")\n",
        "\n",
        "# Collect features\n",
        "features = bestFeaturesDf.select(['features']).collect()\n",
        "\n",
        "# Split\n",
        "train,test = bestFeaturesDf.randomSplit([0.8,0.2])\n",
        "\n",
        "# Specify folds\n",
        "folds = 3\n",
        "\n",
        "#set up your results table\n",
        "columns = ['Classifier', 'Result']\n",
        "vals = [(\"Place Holder\",\"N/A\")]\n",
        "results = spark.createDataFrame(vals, columns)\n",
        "\n",
        "for classifier in classifiers:\n",
        "    new_result = ClassTrainEval(classifier,features,classes,folds,train,test)\n",
        "    results = results.union(new_result)\n",
        "results = results.where(\"Classifier!='Place Holder'\")\n",
        "results.show(100,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grN36c-kdTFE",
        "outputId": "4fbb5d22-838a-4bce-bcfa-436184b01fca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\u001b[1mLogisticRegression\u001b[0m\n",
            "Intercept: [-2.356111774406408]\n",
            "\u001b[1m Top 20 Coefficients\u001b[0m\n",
            "You should compares these relative to eachother\n",
            "+--------+-------------------+\n",
            "|feature |coeff              |\n",
            "+--------+-------------------+\n",
            "|ca      |3.678422996583948  |\n",
            "|trestbps|2.699170945160148  |\n",
            "|oldpeak |2.6592866115283442 |\n",
            "|thal    |1.9459298967384346 |\n",
            "|sex     |1.8605897968595775 |\n",
            "|chol    |1.5764115648037664 |\n",
            "|exang   |1.2937595766874295 |\n",
            "|fbs     |0.1702545825901682 |\n",
            "|slope   |-0.5005313047700715|\n",
            "|age     |-1.059879277424956 |\n",
            "|restecg |-1.354480814603684 |\n",
            "|thalach |-3.0914267538146754|\n",
            "|cp      |-3.3153881537251593|\n",
            "+--------+-------------------+\n",
            "\n",
            "None\n",
            "+------------------+------+\n",
            "|Classifier        |Result|\n",
            "+------------------+------+\n",
            "|LogisticRegression|82.35 |\n",
            "+------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flyc-oNtdWhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}